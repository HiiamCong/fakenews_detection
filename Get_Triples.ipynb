{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ky/anaconda3/envs/thxldll/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/ky/anaconda3/envs/thxldll/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/ky/anaconda3/envs/thxldll/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f18e5f8df28>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import neuralcoref\n",
    "from pycorenlp import *\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "standford_nlp = StanfordCoreNLP(\"http://localhost:9000/\")\n",
    "caps = \"([A-Z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    if text.strip().endswith(\"<stop>\"):\n",
    "        sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralcorefIt(text):\n",
    "    sentences = split_into_sentences(text)\n",
    "#     sentences[0] = sentences[0].capitalize()\n",
    "    for s in sentences:\n",
    "        if s[-1] == '?':\n",
    "            sentences.remove(s)\n",
    "    result = \"\"\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        result += \" \" + doc._.coref_resolved\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPresentTense(triples):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for t in range(0, len(triples)):\n",
    "        for i in range(0,len(triples[t])):\n",
    "            for word in triples[t][i].split(\" \"):\n",
    "                triples[t][i] = triples[t][i].replace(word, lmtzr.lemmatize(word,'v'))\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produceTriples(text):\n",
    "    processedText = neuralcorefIt(text)\n",
    "    output = standford_nlp.annotate(processedText,\n",
    "                          properties={\"annotators\":\"tokenize,ssplit,pos,lemma,depparse,natlog,openie,dcoref\",\n",
    "                                      \"outputFormat\": \"json\",\"triple.strict\":\"true\"})\n",
    "    triples = []\n",
    "    result2 = []\n",
    "    result = [output[\"sentences\"][0][\"openie\"] for item in output]\n",
    "    for i in range(0,len(output[\"sentences\"])):\n",
    "        result2.append(output['sentences'][i]['openie'])\n",
    "        for i in range(0, len(result2)):\n",
    "            for rel in result2[i]:\n",
    "                subj = rel['subject']\n",
    "                obj = rel['object']\n",
    "                for ref in output['corefs']:\n",
    "                    if len(output['corefs'][ref]) > 1 and output['corefs'][ref][1]['position'][0] == i + 1:\n",
    "                        if output['corefs'][ref][1]['text'] == subj:\n",
    "                            subj = output['corefs'][ref][0]['text']\n",
    "                relationSent=[subj,obj,rel['relation']]\n",
    "                triples.append(relationSent)\n",
    "    return getPresentTense(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(triples):\n",
    "    out_put_file = \"data/triples.txt\"\n",
    "    file_out = open(out_put_file, 'w')\n",
    "    check = []\n",
    "    for triple in triples:\n",
    "        line = \"\\t\".join(triple)\n",
    "        if line not in check:\n",
    "            check.append(line)\n",
    "            file_out.write(line.lower() + \"\\n\")\n",
    "    file_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_to_file(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTriples():\n",
    "    sentence = \"Obama was dead. I am so sad for him.\"\n",
    "    triples = produceTriples(sentence)\n",
    "    print(triples)\n",
    "    write_to_file(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addTriples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
